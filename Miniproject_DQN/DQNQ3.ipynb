{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the dqn for me using pytorch and openai gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "import json\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "\n",
    "# get the cuda\n",
    "# import optimimzer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.linear4 = nn.Linear(16, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_dim)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x)) \n",
    "        x = torch.relu(self.linear3(x))\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "def int_to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled action : 0\n",
      "Sampled observation\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x370.286 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "action_space        =   spaces.Discrete(2)\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "print(f\"sampled action : {action_space.sample()}\")\n",
    "print(\"Sampled observation\")\n",
    "plt.matshow(observation_space.sample()[1,:,:])\n",
    "plt.show()\n",
    "\n",
    "SCALE = 100\n",
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "ACTION_ISOLATE = 2\n",
    "ACTION_HOSPITAL = 3\n",
    "ACTION_VACCINATE = 4\n",
    "\n",
    "\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { \n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "    elif a == ACTION_ISOLATE:\n",
    "        action['isolation'] = True\n",
    "    elif a == ACTION_VACCINATE:\n",
    "        action['vaccinate'] = True\n",
    "    elif a == ACTION_HOSPITAL:\n",
    "        action['hospital'] = True\n",
    "        \n",
    "    return action\n",
    "    \n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.power(np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities]), 0.25)\n",
    "    dead = SCALE * np.power( np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities]), 0.25)\n",
    "    confined = np.ones_like(dead)*int((dyn.get_action()['confinement']))\n",
    "    return torch.Tensor(np.stack((infected, dead, confined))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) # define the transition tuple\n",
    "\n",
    "class DQN_Agent(Agent):\n",
    "    def __init__(self,  env:Env, epsilon:float = 0.7, batch_size: int = 2048, buffer_size: int = 20000, device = 'cuda', GAMMA : float = 0.9,\n",
    "                 lr : float = 5e-3, policy_net = None, target_net = None, optimizer = None,\n",
    "                # Additionnal parameters to be added here\n",
    "                ):\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory_initial()\n",
    "        self.device = device\n",
    "        self.GAMMA = GAMMA\n",
    "        self.lr = lr\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        torch.save(self.policy_net.state_dict(), savepath)\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        self.policy_net.load_state_dict(torch.load(savepath))\n",
    "        self.target_net.load_state_dict(self.model.state_dict())\n",
    "        pass\n",
    "\n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "    \n",
    "    # the action function for Q3a\n",
    "    def act(self, obs, temp_epsilon):\n",
    "        # write the epsilon-greedy policy here\n",
    "        if random.random() < temp_epsilon:\n",
    "            return int_to_tensor( self.env.action_space.sample()).unsqueeze(0) \n",
    "        else:\n",
    "            return self.policy_net(obs.to(self.device)).max(1)[1].view(1, 1) # return the action with the highest q-value\n",
    "    \n",
    "    # the action functio for Q3b\n",
    "    def act_decay(self, obs, epsilon0, epsilonmin, Tmax, t):\n",
    "        temp_epsilon = np.max([epsilon0*(Tmax-t)/Tmax, epsilonmin])\n",
    "        return self.act(obs, temp_epsilon)\n",
    "        \n",
    "    def memory_initial(self):\n",
    "        # initialize the memory\n",
    "        self.memory = deque([],maxlen= self.buffer_size) # define the memory as a deque of size capacity\n",
    "\n",
    "    def memory_push(self, transition):\n",
    "        # push a transition into the memory\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def memory_sample(self):\n",
    "        # sample a batch from memory\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def memory_len(self):\n",
    "        # return the length of the memory\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def optimize_model(self, train = True):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        if self.memory_len() < self.batch_size:\n",
    "            return 0\n",
    "        transitions = self.memory_sample()\n",
    "\n",
    "        batch = Transition(*zip(*transitions)) # unzip the batch\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device= self.device, dtype=torch.bool).to(device)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        \n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).squeeze().to(device)\n",
    "\n",
    "        action_batch = action_batch.long()\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch).to(device)\n",
    "        next_state_values = torch.zeros(self.batch_size, device= self.device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "                \n",
    "        criterion = nn.MSELoss() # define the loss function\n",
    "\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        if train == True:\n",
    "            # Optimize the model\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # In-place gradient clipping\n",
    "            torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100) \n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_episode(env,agent,random_seed):\n",
    "    state, _ = env.reset(random_seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state, 0.7)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.memory_push(Transition(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model()\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_reward.item(), total_loss\n",
    "\n",
    "def one_episode_decay(env,agent,random_seed, epsilon0, epsilonmin, Tmax, t):\n",
    "    state, _ = env.reset(random_seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act_decay(state, epsilon0, epsilonmin, Tmax, t)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.memory_push(Transition(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model()\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_reward.item(), total_loss\n",
    "\n",
    "\n",
    "def eval_one(env,agent, random_seed):\n",
    "    state, _ = env.reset(random_seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state,0)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model(train = False)\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_reward.item(), total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_seed = 42 \n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.9\n",
    "LR = 5e-3\n",
    "input_dim = 3 * dyn.n_cities * dyn.env_step_length\n",
    "\n",
    "state, info = env.reset(42)\n",
    "\n",
    "# Initialize policy and target network\n",
    "policy_net = DQN(input_dim, 2).to(device)\n",
    "target_net = DQN(input_dim, 2).to(device)\n",
    "# Initialize the target network to be the same as the policy network\n",
    "target_net.load_state_dict(policy_net.state_dict()) \n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "DQNagent = DQN_Agent(env, epsilon=0.7, batch_size=BATCH_SIZE, buffer_size=20000, GAMMA=GAMMA,lr = LR, device=device ,\n",
    "                    policy_net=policy_net, target_net=target_net, optimizer=optimizer)\n",
    "\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 : train_reward -167.0216128680171, train_loss 0.0\n",
      "Episode 50 : eval_reward 21.347885131835938, eval_loss 0.0\n",
      "Episode 100 : train_reward -108.82926143918719, train_loss 676.9717066239338\n",
      "Episode 100 : eval_reward 23.687984466552734, eval_loss 459.5527935028076\n",
      "Episode 150 : train_reward -74.73054054318642, train_loss 228.77520672155887\n",
      "Episode 150 : eval_reward 24.136550903320312, eval_loss 250.15599060058594\n",
      "Episode 200 : train_reward -71.07371185507093, train_loss 216.6966087380234\n",
      "Episode 200 : eval_reward 32.74494171142578, eval_loss 195.02638626098633\n",
      "Episode 250 : train_reward -95.88674496631234, train_loss 281.36063483296607\n",
      "Episode 250 : eval_reward 46.69994354248047, eval_loss 370.2951316833496\n",
      "Episode 300 : train_reward -101.4096159448429, train_loss 302.3044276529429\n",
      "Episode 300 : eval_reward 41.61518859863281, eval_loss 268.0845251083374\n",
      "Episode 350 : train_reward -88.1493259254767, train_loss 310.60403827745085\n",
      "Episode 350 : eval_reward 21.008136749267578, eval_loss 311.7946081161499\n",
      "Episode 400 : train_reward -104.01010359550008, train_loss 322.9261934416635\n",
      "Episode 400 : eval_reward 53.13569641113281, eval_loss 344.88960361480713\n",
      "Episode 450 : train_reward -75.85184548339065, train_loss 353.4416940163593\n",
      "Episode 450 : eval_reward 46.94608688354492, eval_loss 297.535213470459\n",
      "Episode 500 : train_reward -79.44344527380807, train_loss 344.0273545323586\n",
      "Episode 500 : eval_reward 52.054054260253906, eval_loss 336.56360721588135\n"
     ]
    }
   ],
   "source": [
    "def train_Q3a():\n",
    "    train_loss_trace = []\n",
    "    train_reward_trace = []\n",
    "    eval_loss_trace = []\n",
    "    eval_reward_trace = []\n",
    "\n",
    "    best = -np.inf\n",
    "\n",
    "    for i in range(1,501):\n",
    "        reward, loss = one_episode(env,DQNagent,i)\n",
    "        train_loss_trace.append(loss)\n",
    "        train_reward_trace.append(reward)\n",
    "        #print(f\"Episode {i} : reward {reward}, loss {loss}\")\n",
    "        if i % 5 == 0:\n",
    "            DQNagent.update_target_model()\n",
    "        if i % 50 == 0 :\n",
    "            tuple_list = np.array([eval_one(env,DQNagent,i) for m in range(20)])\n",
    "            eval_losss = np.mean(tuple_list,axis = 0)[1]\n",
    "            eval_rewards = np.mean(tuple_list,axis = 0)[0]\n",
    "\n",
    "            eval_loss_trace.append(eval_losss)\n",
    "            eval_reward_trace.append(eval_rewards)\n",
    "            train_loss = np.mean(train_loss_trace[i-50:i-1])\n",
    "            train_reward = np.mean(train_reward_trace[i-50:i-1])\n",
    "            \n",
    "            if eval_rewards > best:\n",
    "                best = eval_rewards\n",
    "                torch.save(DQNagent.policy_net.state_dict(), \"DQN_best_Q3a.pth\")\n",
    "            print(f\"Episode {i} : train_reward {train_reward}, train_loss {train_loss}\")\n",
    "            print(f\"Episode {i} : eval_reward {eval_rewards}, eval_loss {eval_losss}\")\n",
    "\n",
    "    with open('Q3a_train_loss.json', 'w') as f: json.dump(train_loss_trace, f)\n",
    "    with open('Q3a_train_reward.json', 'w') as f: json.dump(train_reward_trace, f)\n",
    "    with open('Q3a_eval_loss.json', 'w') as f: json.dump(eval_loss_trace, f)\n",
    "    with open('Q3a_eval_reward.json', 'w') as f: json.dump(eval_reward_trace, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNagent.load_model(\"DQN_best_Q3a.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNagentb = DQN_Agent(env, epsilon=0.7, batch_size=BATCH_SIZE, buffer_size=20000, GAMMA=GAMMA,lr = LR, device=device ,\n",
    "                    policy_net=policy_net, target_net=target_net, optimizer=optimizer)\n",
    "\n",
    "def train_Q3b():\n",
    "    train_loss_trace = []\n",
    "    train_reward_trace = []\n",
    "    eval_loss_trace = []\n",
    "    eval_reward_trace = []\n",
    "\n",
    "    best = -np.inf\n",
    "\n",
    "    for i in range(1,501):\n",
    "        reward, loss = one_episode_decay(env,DQNagentb,i,0.7,0.2,500,i)\n",
    "        train_loss_trace.append(loss)\n",
    "        train_reward_trace.append(reward)\n",
    "        #print(f\"Episode {i} : reward {reward}, loss {loss}\")\n",
    "        if i % 5 == 0:\n",
    "            DQNagent.update_target_model()\n",
    "        if i % 50 == 0 :\n",
    "            tuple_list = np.array([eval_one(env,DQNagent,i) for m in range(20)])\n",
    "            eval_losss = np.mean(tuple_list,axis = 0)[1]\n",
    "            eval_rewards = np.mean(tuple_list,axis = 0)[0]\n",
    "\n",
    "            eval_loss_trace.append(eval_losss)\n",
    "            eval_reward_trace.append(eval_rewards)\n",
    "            train_loss = np.mean(train_loss_trace[i-50:i-1])\n",
    "            train_reward = np.mean(train_reward_trace[i-50:i-1])\n",
    "            \n",
    "            if eval_rewards > best:\n",
    "                best = eval_rewards\n",
    "                torch.save(DQNagent.policy_net.state_dict(), \"DQN_best_Q3b.pth\")\n",
    "            print(f\"Episode {i} : train_reward {train_reward}, train_loss {train_loss}\")\n",
    "            print(f\"Episode {i} : eval_reward {eval_rewards}, eval_loss {eval_losss}\")\n",
    "\n",
    "    with open('Q3b_train_loss.json', 'w') as f: json.dump(train_loss_trace, f)\n",
    "    with open('Q3b_train_reward.json', 'w') as f: json.dump(train_reward_trace, f)\n",
    "    with open('Q3b_eval_loss.json', 'w') as f: json.dump(eval_loss_trace, f)\n",
    "    with open('Q3b_eval_reward.json', 'w') as f: json.dump(eval_reward_trace, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
