{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the dqn for me using pytorch and openai gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "\n",
    "# get the cuda\n",
    "# import optimimzer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.linear4 = nn.Linear(16, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_dim)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x)) \n",
    "        x = torch.relu(self.linear3(x))\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "def int_to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled action : 1\n",
      "Sampled observation\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x370.286 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "action_space        =   spaces.Discrete(2)\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "print(f\"sampled action : {action_space.sample()}\")\n",
    "print(\"Sampled observation\")\n",
    "plt.matshow(observation_space.sample()[1,:,:])\n",
    "plt.show()\n",
    "\n",
    "SCALE = 100\n",
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "ACTION_ISOLATE = 2\n",
    "ACTION_HOSPITAL = 3\n",
    "ACTION_VACCINATE = 4\n",
    "\n",
    "\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { \n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "    elif a == ACTION_ISOLATE:\n",
    "        action['isolation'] = True\n",
    "    elif a == ACTION_VACCINATE:\n",
    "        action['vaccinate'] = True\n",
    "    elif a == ACTION_HOSPITAL:\n",
    "        action['hospital'] = True\n",
    "        \n",
    "    return action\n",
    "    \n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.power(np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities]), 0.25)\n",
    "    dead = SCALE * np.power( np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities]), 0.25)\n",
    "    confined = np.ones_like(dead)*int((dyn.get_action()['confinement']))\n",
    "    return torch.Tensor(np.stack((infected, dead, confined))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) # define the transition tuple\n",
    "\n",
    "class DQN_Agent(Agent):\n",
    "    def __init__(self,  env:Env, epsilon:float = 0.7, batch_size: int = 2048, buffer_size: int = 20000, device = 'cuda', GAMMA : float = 0.9,\n",
    "                 lr : float = 5e-3, policy_net = None, target_net = None, optimizer = None,\n",
    "                # Additionnal parameters to be added here\n",
    "                ):\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory_initial()\n",
    "        self.device = device\n",
    "        self.GAMMA = GAMMA\n",
    "        self.lr = lr\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        pass\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "    \n",
    "    def act(self, obs):\n",
    "        # write the epsilon-greedy policy here\n",
    "        if random.random() < self.epsilon:\n",
    "            return int_to_tensor( self.env.action_space.sample()).unsqueeze(0) \n",
    "        else:\n",
    "            return self.policy_net(obs.to(self.device)).max(1)[1].view(1, 1) # return the action with the highest q-value\n",
    "    \n",
    "    def memory_initial(self):\n",
    "        # initialize the memory\n",
    "        self.memory = deque([],maxlen= self.buffer_size) # define the memory as a deque of size capacity\n",
    "\n",
    "    def memory_push(self, transition):\n",
    "        # push a transition into the memory\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def memory_sample(self):\n",
    "        # sample a batch from memory\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def memory_len(self):\n",
    "        # return the length of the memory\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        if self.memory_len() < self.batch_size:\n",
    "            return 0\n",
    "        transitions = self.memory_sample()\n",
    "\n",
    "        batch = Transition(*zip(*transitions)) # unzip the batch\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device= self.device, dtype=torch.bool).to(device)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        \n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).squeeze().to(device)\n",
    "\n",
    "        action_batch = action_batch.long()\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch).to(device)\n",
    "        next_state_values = torch.zeros(self.batch_size, device= self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "                \n",
    "        criterion = nn.MSELoss() # define the loss function\n",
    "\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100) \n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_episode(env,agent,random_seed):\n",
    "    state, _ = env.reset(random_seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.memory_push(Transition(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model()\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_reward.item(), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_seed = 42 \n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.9\n",
    "LR = 5e-3\n",
    "input_dim = 3 * dyn.n_cities * dyn.env_step_length\n",
    "\n",
    "state, info = env.reset(42)\n",
    "\n",
    "# Initialize policy and target network\n",
    "policy_net = DQN(input_dim, 2).to(device)\n",
    "target_net = DQN(input_dim, 2).to(device)\n",
    "# Initialize the target network to be the same as the policy network\n",
    "target_net.load_state_dict(policy_net.state_dict()) \n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "DQNagent = DQN_Agent(env, epsilon=0.7, batch_size=BATCH_SIZE, buffer_size=20000, GAMMA=GAMMA,lr = LR, device=device ,\n",
    "                    policy_net=policy_net, target_net=target_net, optimizer=optimizer)\n",
    "\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 : reward -97.8452377319336, loss 0\n",
      "Episode 1 : reward -69.86727142333984, loss 0\n",
      "Episode 2 : reward -149.11497497558594, loss 0\n",
      "Episode 3 : reward -78.35105895996094, loss 0\n",
      "Episode 4 : reward -53.041168212890625, loss 0\n",
      "Episode 5 : reward -220.19692993164062, loss 0\n",
      "Episode 6 : reward -333.8187255859375, loss 0\n",
      "Episode 7 : reward -32.44615173339844, loss 0\n",
      "Episode 8 : reward -82.30399322509766, loss 0\n",
      "Episode 9 : reward -194.16038513183594, loss 0\n",
      "Episode 10 : reward -87.20501708984375, loss 0\n",
      "Episode 11 : reward -89.59663391113281, loss 0\n",
      "Episode 12 : reward -285.17608642578125, loss 0\n",
      "Episode 13 : reward -39.501129150390625, loss 0\n",
      "Episode 14 : reward -178.9618377685547, loss 0\n",
      "Episode 15 : reward -57.09097671508789, loss 0\n",
      "Episode 16 : reward -176.8500213623047, loss 0\n",
      "Episode 17 : reward -96.51532745361328, loss 0\n",
      "Episode 18 : reward -223.2037353515625, loss 0\n",
      "Episode 19 : reward -114.81475067138672, loss 0\n",
      "Episode 20 : reward -308.222900390625, loss 0\n",
      "Episode 21 : reward -108.49636840820312, loss 0\n",
      "Episode 22 : reward -175.18649291992188, loss 0\n",
      "Episode 23 : reward -129.1703643798828, loss 0\n",
      "Episode 24 : reward -160.97329711914062, loss 0\n",
      "Episode 25 : reward -96.77987670898438, loss 0\n",
      "Episode 26 : reward -111.46636962890625, loss 0\n",
      "Episode 27 : reward -12.558650970458984, loss 0\n",
      "Episode 28 : reward -161.3213348388672, loss 0\n",
      "Episode 29 : reward -184.38290405273438, loss 0\n",
      "Episode 30 : reward -68.92561340332031, loss 0\n",
      "Episode 31 : reward -116.88609313964844, loss 0\n",
      "Episode 32 : reward -76.37419891357422, loss 0\n",
      "Episode 33 : reward -55.04016876220703, loss 0\n",
      "Episode 34 : reward -102.16970825195312, loss 0\n",
      "Episode 35 : reward -183.09774780273438, loss 0\n",
      "Episode 36 : reward 27.218063354492188, loss 0\n",
      "Episode 37 : reward -160.48239135742188, loss 0\n",
      "Episode 38 : reward -40.71451187133789, loss 0\n",
      "Episode 39 : reward -110.57032012939453, loss 0\n",
      "Episode 40 : reward -267.33428955078125, loss 0\n",
      "Episode 41 : reward -96.99164581298828, loss 0\n",
      "Episode 42 : reward -122.49750518798828, loss 0\n",
      "Episode 43 : reward -116.50578308105469, loss 0\n",
      "Episode 44 : reward -131.8477783203125, loss 0\n",
      "Episode 45 : reward -22.741188049316406, loss 0\n",
      "Episode 46 : reward -182.11427307128906, loss 0\n",
      "Episode 47 : reward -225.6353302001953, loss 0\n",
      "Episode 48 : reward -107.67984008789062, loss 0\n",
      "Episode 49 : reward 12.912344932556152, loss 0\n",
      "Episode 50 : reward -89.84998321533203, loss 0\n",
      "Episode 51 : reward 27.322839736938477, loss 0\n",
      "Episode 52 : reward -122.43031311035156, loss 0\n",
      "Episode 53 : reward 12.995262145996094, loss 0\n",
      "Episode 54 : reward -107.04378509521484, loss 0\n",
      "Episode 55 : reward -182.23739624023438, loss 0\n",
      "Episode 56 : reward -83.37716674804688, loss 0\n",
      "Episode 57 : reward -69.88296508789062, loss 0\n",
      "Episode 58 : reward -96.68492889404297, loss 0\n",
      "Episode 59 : reward -13.638662338256836, loss 0\n",
      "Episode 60 : reward -39.46059036254883, loss 0\n",
      "Episode 61 : reward -136.1479949951172, loss 0\n",
      "Episode 62 : reward -127.77963256835938, loss 0\n",
      "Episode 63 : reward -111.07543182373047, loss 0\n",
      "Episode 64 : reward -236.93234252929688, loss 0\n",
      "Episode 65 : reward -250.52090454101562, loss 0\n",
      "Episode 66 : reward -68.2002182006836, loss 0\n",
      "Episode 67 : reward -119.66932678222656, loss 0\n",
      "Episode 68 : reward -209.54129028320312, loss 4494.204246520996\n",
      "Episode 69 : reward -179.39646911621094, loss 2746.2018127441406\n",
      "Episode 70 : reward -104.53860473632812, loss 1685.9502563476562\n",
      "Episode 71 : reward -150.47018432617188, loss 2846.8720779418945\n",
      "Episode 72 : reward -82.99369049072266, loss 1576.7573204040527\n",
      "Episode 73 : reward -144.66192626953125, loss 1132.0685424804688\n",
      "Episode 74 : reward -124.57546997070312, loss 851.9350891113281\n",
      "Episode 75 : reward -54.52178192138672, loss 656.5641803741455\n",
      "Episode 76 : reward -78.44466400146484, loss 756.093183517456\n",
      "Episode 77 : reward 38.34597396850586, loss 559.7387247085571\n",
      "Episode 78 : reward -64.01287078857422, loss 477.0051202774048\n",
      "Episode 79 : reward -115.83831787109375, loss 424.380859375\n",
      "Episode 80 : reward 1.2339800596237183, loss 396.3097553253174\n",
      "Episode 81 : reward -54.803653717041016, loss 580.2186403274536\n",
      "Episode 82 : reward -43.341304779052734, loss 383.22416400909424\n",
      "Episode 83 : reward -195.40602111816406, loss 358.17590045928955\n",
      "Episode 84 : reward -145.67742919921875, loss 373.22411251068115\n",
      "Episode 85 : reward -63.9168586730957, loss 331.3338623046875\n",
      "Episode 86 : reward -21.211538314819336, loss 598.3741865158081\n",
      "Episode 87 : reward -122.79487609863281, loss 337.1921033859253\n",
      "Episode 88 : reward -152.30970764160156, loss 323.287654876709\n",
      "Episode 89 : reward -102.59344482421875, loss 296.27918910980225\n",
      "Episode 90 : reward -22.826019287109375, loss 306.6059093475342\n",
      "Episode 91 : reward -7.356019973754883, loss 392.6767921447754\n",
      "Episode 92 : reward -162.0677490234375, loss 296.76035499572754\n",
      "Episode 93 : reward -174.65655517578125, loss 313.9399576187134\n",
      "Episode 94 : reward -225.00672912597656, loss 311.4188222885132\n",
      "Episode 95 : reward -238.86602783203125, loss 319.82866191864014\n",
      "Episode 96 : reward -31.743728637695312, loss 375.6580648422241\n",
      "Episode 97 : reward -1.982170820236206, loss 339.79540061950684\n",
      "Episode 98 : reward -248.4410858154297, loss 307.28130292892456\n",
      "Episode 99 : reward -66.409423828125, loss 343.2021493911743\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    reward, loss = one_episode(env,DQNagent,i)\n",
    "    print(f\"Episode {i} : reward {reward}, loss {loss}\")\n",
    "    if i % 5 == 0:\n",
    "        DQNagent.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
