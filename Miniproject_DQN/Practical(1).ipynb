{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8611109-c662-4c2b-bb43-943b101e9d4a",
   "metadata": {},
   "source": [
    "## Mini Project 1\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654a51a8-9f6a-4789-92b7-69940c8aee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db47073-3c03-4277-a033-6077a1687b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lausanne', 'Geneva', 'Sion', 'Neuchâtel', 'Basel', 'Bern', 'Lücern', 'St-Gallen', 'Zürich']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the switzerland map\n",
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "print(dyn.cities)\n",
    "dyn.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea9d5b-a7ee-4844-b848-4950df644723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(log):\n",
    "    \"\"\" Divide the log into the different cities and plot the infected and dead people for each city \"\"\"\n",
    "    total = {p: np.array([getattr(l.total, p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c: {p: np.array([getattr(l.city[c], p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a: np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "\n",
    "    \"\"\" Plot the full state, the observable state and the actions\"\"\"\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "    ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "    ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "    ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "    ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "    ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "    [ax_leftstate.plot(y) for y in total.values()]\n",
    "    ax_leftstate.legend(total.keys())\n",
    "    ax_leftstate.set_title('Full state')\n",
    "    ax_leftstate.set_ylabel('number of people in each state')\n",
    "\n",
    "    [ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "    ax_leftobs.legend(['infected','dead'])\n",
    "    ax_leftobs.set_title('Observable state')\n",
    "    ax_leftobs.set_ylabel('number of people in each state')\n",
    "\n",
    "    ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "    ax_leftactions.set_title('Actions')\n",
    "    ax_leftactions.set_yticks([0,1,2,3])\n",
    "    ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "    ax_leftactions.set_xlabel('time (in weeks)')\n",
    "\n",
    "    [ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "    [ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "    [ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "    [ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "    ax_right['Zürich'].set_xlabel('time (in weeks)')\n",
    "    ax_right['Zürich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(log, action=True):\n",
    "    \"\"\" Divide the log into the different cities and plot the infected and dead people for each city \"\"\"\n",
    "    total = {p: np.array([getattr(l.total, p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c: {p: np.array([getattr(l.city[c], p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a: np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "\n",
    "    \"\"\" Plot the full state, the observable state and the actions\"\"\"\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "    ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "    ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2) if action else None\n",
    "    ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "    ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "    ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "    [ax_leftstate.plot(y) for y in total.values()]\n",
    "    ax_leftstate.legend(total.keys())\n",
    "    ax_leftstate.set_title('Full state')\n",
    "    ax_leftstate.set_ylabel('number of people in each state')\n",
    "\n",
    "    [ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "    ax_leftobs.legend(['infected','dead'])\n",
    "    ax_leftobs.set_title('Observable state')\n",
    "    ax_leftobs.set_ylabel('number of people in each state')\n",
    "\n",
    "    if action:\n",
    "        ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "        ax_leftactions.set_title('Actions')\n",
    "        ax_leftactions.set_yticks([0,1,2,3])\n",
    "        ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "        ax_leftactions.set_xlabel('time (in weeks)')\n",
    "\n",
    "    [ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "    [ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "    [ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "    [ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "    ax_right['Zürich'].set_xlabel('time (in weeks)')\n",
    "    ax_right['Zürich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bac7ff39-30b0-463e-b613-97c1b5ed0dde",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115a8495-fecb-46a3-8016-95e2eef46c5e",
   "metadata": {},
   "source": [
    "#### Question 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c291c-a1f4-4a05-a794-485e9471360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = Env(dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ec127-45c8-420b-8a77-485bf13b6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement: Using the same sequence of random seeds.\n",
    "seed = 42\n",
    "\n",
    "\n",
    "unmitigated = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed=seed)\n",
    "while not finished:\n",
    "    action = unmitigated\n",
    "    obs, reward, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc3539-7a38-413e-9f23-786c0792014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log,action=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f10db752",
   "metadata": {},
   "source": [
    "##  Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be30ff9-4371-4ac5-aa7b-1e4f102a5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Pr. Russo's Policy in the form of python class\"\"\"\n",
    "class Russo(Agent):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.confine_index = 0\n",
    "    \n",
    "    def load_model(self, savepath):\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self):\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.confine_index = 0\n",
    "    \n",
    "    def act(self, obs):\n",
    "        # If the action is not confinement, and infected people are more than 20000, then confine for 4 weeks\n",
    "        if self.confine_index == 0 and obs.total.infected[-1] > 20000:\n",
    "            self.confine_index = 4\n",
    "            \n",
    "        # Action for the weeks of confinement\n",
    "        if self.confine_index > 0:\n",
    "            self.confine_index -= 1\n",
    "            confine_state = True\n",
    "        else:\n",
    "            confine_state = False\n",
    "\n",
    "        action = {\n",
    "            'confinement': confine_state,\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }\n",
    "        \n",
    "        return action\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(dyn)\n",
    "agent = Russo(env)\n",
    "seed = 42\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed=seed)\n",
    "while not finished:\n",
    "    action = agent.act(obs)\n",
    "    obs, reward, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ceba2bf",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3df942e",
   "metadata": {},
   "source": [
    "### Question 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Evaluate the policy, save confined day, reward, and total death. For each episode, save the mentioned values\"\"\"\n",
    "def evaluate(agent, env, episodes=50,seed=42):\n",
    "    log_confine = []\n",
    "    log_reward = []\n",
    "    log_death = []\n",
    "\n",
    "    # Run the episodes\n",
    "    np.random.seed(seed)\n",
    "    seed_sequence = np.random.randint(0, 1000, size=episodes)\n",
    "    for i in range(episodes): \n",
    "        # Reset the episode\n",
    "        episode_confine = 0\n",
    "        episode_reward = 0\n",
    "        episode_death = 0\n",
    "        obs, info = env.reset(seed_sequence[i])\n",
    "        agent.reset()\n",
    "\n",
    "        # Run the episode\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            action = agent.act(obs)\n",
    "            obs, reward, finished, info = env.step(action)\n",
    "            episode_confine += 7 * int(info.action['confinement'])# weeks to 7 days\n",
    "            episode_reward += float(reward) # reward is a tensor, convert it to float, or the figure cannot be plotted\n",
    "            episode_death = int(info.total.dead)\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Save the episode\n",
    "        log_confine.append(episode_confine)\n",
    "        log_reward.append(episode_reward)\n",
    "        log_death.append(episode_death)\n",
    "\n",
    "    return log_confine, log_reward, log_death\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot via histogram \"\"\"\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n",
    "    ax.grid()\n",
    "\n",
    "def plot_hist(log_confine, log_reward, log_death):\n",
    "    fig, ax = plt.subplots(3,1,figsize=(15,12))\n",
    "    hist_avg(ax[0],log_confine,'confinement days')\n",
    "    hist_avg(ax[1],log_reward,'cumulative rewards')\n",
    "    hist_avg(ax[2],log_death,'deaths')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd223eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "env = Env(dyn)\n",
    "agent = Russo(env)\n",
    "\n",
    "log_confine, log_reward, log_death = evaluate(agent, env, episodes=50,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(log_confine, log_reward, log_death)\n",
    "\n",
    "\"\"\"Print example of the log\"\"\"\n",
    "print(f'Average death: {np.mean(log_death)}')\n",
    "print(f'Average reward: {np.mean(log_reward)}')\n",
    "print(f'Average confined days: {np.mean(log_confine)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d73d94e",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b379b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42724871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from gym import spaces\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1007e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dafcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e988087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled space: 1\n",
      "Sampled observation\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x370.286 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the action space\n",
    "action_space = spaces.Discrete(2) # 0: False, 1: True\n",
    "observation_space = spaces.Box(low = 0,\n",
    "                               high = 1,\n",
    "                               shape = (2, dyn.n_cities,dyn.env_step_length),\n",
    "                               dtype = np.float16)\n",
    "print(f'sampled space: {action_space.sample()}')\n",
    "print(\"Sampled observation\")\n",
    "plt.matshow(observation_space.sample()[1,:,:])\n",
    "plt.show()\n",
    "\n",
    "SCALE = 100\n",
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "ACTION_ISOLATE = 2\n",
    "ACTION_HOSPITAL = 3\n",
    "ACTION_VACCINATE = 4\n",
    "\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor,dyn: ModelDynamics):\n",
    "    action = {\n",
    "        'confinement': False,\n",
    "        'isolation': False,\n",
    "        'hospital': False,\n",
    "        'vaccinate': False,\n",
    "    }\n",
    "\n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "    elif a == ACTION_ISOLATE:\n",
    "        action['isolation'] = True\n",
    "    elif a == ACTION_VACCINATE:\n",
    "        action['vaccinate'] = True\n",
    "    elif a == ACTION_HOSPITAL:\n",
    "        action['hospital'] = True\n",
    "\n",
    "    return action\n",
    "\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.power(np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities]),0.25)\n",
    "    dead = SCALE * np.power(np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities]),0.25)\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb288b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b958ac",
   "metadata": {},
   "source": [
    "### Question 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5f19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        hidden_size1 = 64\n",
    "        hidden_size2 = 32\n",
    "        hidden_size3 = 16\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.layer4 = nn.Linear(hidden_size3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.input_size)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8732af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_tensor(x, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95758575",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b55b1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent): \n",
    "\n",
    "    def __init__(self, env:Env, batch_size: int =2048, \n",
    "                 lr:float = 5e-3, buffer_size: int = 20000,  gamma:float = 0.9, \n",
    "                 epsilon:float = 0.7, target_net = None,\n",
    "                 device = device, policy_net = None, optimizer = None,\n",
    "                 ):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.policy_net = policy_net\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory_initial()\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "\n",
    "    \"\"\"ReplayMemory Part Put Here To Avoid Confusion\"\"\"\n",
    "    def memory_initial(self):\n",
    "        self.memory = deque([],maxlen= self.buffer_size)\n",
    "\n",
    "    def memory_push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def memory_sample(self,):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "    def memory_len(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \"\"\"Agent Part\"\"\"\n",
    "    def load_model(self, savepath):\n",
    "        self.policy_net.load_state_dict(torch.load(savepath))\n",
    "        self.target_net.load_state_dict(self.model.state_dict())\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        torch.save(self.policy_net.state_dict(), savepath)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, obs, epsilon_fixed): \n",
    "        # epsilon greedy\n",
    "        if random.random() < epsilon_fixed:\n",
    "            return int_to_tensor( self.env.action_space.sample()).unsqueeze(0) \n",
    "        else:\n",
    "            return self.policy_net(obs.to(self.device)).max(1)[1].view(1, 1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def optimize_model(self, train = True):\n",
    "        if self.memory_len() < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        transitions = self.memory_sample()\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=self.device, dtype=torch.bool).to(device)\n",
    "        \n",
    "        non_final_next_state = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "        \n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).squeeze().to(device)\n",
    "\n",
    "        action_batch = action_batch.long()\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch).to(device)\n",
    "        \n",
    "\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_state).max(1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Computer MSE loss\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "        if train == True:\n",
    "            # Optimize the model\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8532d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_episode(env, agent, seed):\n",
    "    state, _ = env.reset(seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state, 0.7)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.memory_push(Transition(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model()\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_reward.item(), total_loss\n",
    "\n",
    "\n",
    "def each_evaluation(env, agent, seed):\n",
    "    state, _ = env.reset(seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        loss = agent.optimize_model(train = False)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_reward.item(), total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b125f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 2048\n",
    "lr = 5e-3\n",
    "buffer_size = 20000\n",
    "gamma = 0.9\n",
    "input_size = 2 * dyn.n_cities * dyn.env_step_length\n",
    "\n",
    "state, info = env.reset(seed)\n",
    "\n",
    "# initialize policy network\n",
    "policy_net = DQN(input_size,2).to(device)\n",
    "target_net = DQN(input_size,2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "dqnagent_3a = DQNAgent(env, gamma= gamma, epsilon= 0.7, policy_net = policy_net, target_net = target_net, device = device,optimizer=optimizer)\n",
    "\n",
    "n_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e901a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3a(training_loss_trace_path = None,training_rewards_trace_path=  None, eval_loss_trace_path = None, eval_rewards_trace_path = None,): \n",
    "\n",
    "    train_length:int = 500\n",
    "    full_update: int = 5\n",
    "    small_cycle: int = 50\n",
    "\n",
    "\n",
    "    training_loss_trace = []\n",
    "    training_rewards_trace = []\n",
    "    eval_loss_trace = []\n",
    "    eval_rewards_trace = []\n",
    "\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    for i in range(1, train_length + 1):\n",
    "        if i == 68: \n",
    "            print(\"pasuse\")\n",
    "        reward, loss = each_episode(env, dqnagent_3a, i)\n",
    "        training_loss_trace.append(loss)\n",
    "        training_rewards_trace.append(reward)\n",
    "        print('Episode: ', i, '| Episode Reward: ', reward, '| Episode Loss: ', loss)\n",
    "        if i % full_update == 0:\n",
    "            # Fully update the target network every 5 episodes\n",
    "            dqnagent_3a.update_target_model()\n",
    "        \n",
    "        if i % small_cycle == 0:\n",
    "            eval_list = np.array([each_evaluation(env,dqnagent_3a,i) for _ in range(20)])\n",
    "            eval_loss = np.mean(eval_list,axis = 0)[1]\n",
    "            eval_reward = np.mean(eval_list,axis = 0)[0]\n",
    "\n",
    "            eval_loss_trace.append(eval_loss)\n",
    "            eval_rewards_trace.append(eval_reward)\n",
    "            training_loss = np.mean(training_loss_trace[i-50:i-1])\n",
    "            training_reward = np.mean(training_rewards_trace[i-50:i-1])\n",
    "\n",
    "            if eval_reward > best_reward:\n",
    "                best_reward = eval_reward\n",
    "                torch.save(dqnagent_3a.policy_net.state_dict(), '3a_dqnagent_best.pth')\n",
    "            # print episode i / training_length\n",
    "            print(f'Episode {i} of {train_length}:train reward {training_reward:.2f}, train loss {training_loss:.4f}, eval reward {eval_reward:.2f},eval loss {eval_loss:.2f}')\n",
    "\n",
    "    with open(training_loss_trace_path, 'w') as f:\n",
    "        json.dump(training_loss_trace, f)\n",
    "    with open(training_rewards_trace_path, 'w') as f:\n",
    "        json.dump(training_rewards_trace, f)\n",
    "    with open(eval_loss_trace_path, 'w') as f:\n",
    "        json.dump(eval_loss_trace, f)\n",
    "    with open(eval_rewards_trace_path, 'w') as f:\n",
    "        json.dump(eval_rewards_trace, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "502f77bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1 | Episode Reward:  -198.63919067382812 | Episode Loss:  0\n",
      "Episode:  2 | Episode Reward:  -213.29391479492188 | Episode Loss:  0\n",
      "Episode:  3 | Episode Reward:  -172.13829040527344 | Episode Loss:  0\n",
      "Episode:  4 | Episode Reward:  -276.7890319824219 | Episode Loss:  0\n",
      "Episode:  5 | Episode Reward:  -93.92024993896484 | Episode Loss:  0\n",
      "Episode:  6 | Episode Reward:  -305.8403625488281 | Episode Loss:  0\n",
      "Episode:  7 | Episode Reward:  -144.24319458007812 | Episode Loss:  0\n",
      "Episode:  8 | Episode Reward:  -79.31829833984375 | Episode Loss:  0\n",
      "Episode:  9 | Episode Reward:  -198.124755859375 | Episode Loss:  0\n",
      "Episode:  10 | Episode Reward:  -132.19419860839844 | Episode Loss:  0\n",
      "Episode:  11 | Episode Reward:  -127.28489685058594 | Episode Loss:  0\n",
      "Episode:  12 | Episode Reward:  -138.8107147216797 | Episode Loss:  0\n",
      "Episode:  13 | Episode Reward:  -115.83476257324219 | Episode Loss:  0\n",
      "Episode:  14 | Episode Reward:  -151.30810546875 | Episode Loss:  0\n",
      "Episode:  15 | Episode Reward:  -202.2118682861328 | Episode Loss:  0\n",
      "Episode:  16 | Episode Reward:  -135.30978393554688 | Episode Loss:  0\n",
      "Episode:  17 | Episode Reward:  -136.47486877441406 | Episode Loss:  0\n",
      "Episode:  18 | Episode Reward:  -227.7615203857422 | Episode Loss:  0\n",
      "Episode:  19 | Episode Reward:  -116.83966064453125 | Episode Loss:  0\n",
      "Episode:  20 | Episode Reward:  -338.55938720703125 | Episode Loss:  0\n",
      "Episode:  21 | Episode Reward:  -299.3857116699219 | Episode Loss:  0\n",
      "Episode:  22 | Episode Reward:  -262.4017333984375 | Episode Loss:  0\n",
      "Episode:  23 | Episode Reward:  -180.83230590820312 | Episode Loss:  0\n",
      "Episode:  24 | Episode Reward:  -113.9752197265625 | Episode Loss:  0\n",
      "Episode:  25 | Episode Reward:  -203.20448303222656 | Episode Loss:  0\n",
      "Episode:  26 | Episode Reward:  -67.86788177490234 | Episode Loss:  0\n",
      "Episode:  27 | Episode Reward:  -295.0057678222656 | Episode Loss:  0\n",
      "Episode:  28 | Episode Reward:  -149.68772888183594 | Episode Loss:  0\n",
      "Episode:  29 | Episode Reward:  -266.5136413574219 | Episode Loss:  0\n",
      "Episode:  30 | Episode Reward:  -145.89024353027344 | Episode Loss:  0\n",
      "Episode:  31 | Episode Reward:  -151.08595275878906 | Episode Loss:  0\n",
      "Episode:  32 | Episode Reward:  -153.43057250976562 | Episode Loss:  0\n",
      "Episode:  33 | Episode Reward:  -179.46981811523438 | Episode Loss:  0\n",
      "Episode:  34 | Episode Reward:  -164.54998779296875 | Episode Loss:  0\n",
      "Episode:  35 | Episode Reward:  -171.82125854492188 | Episode Loss:  0\n",
      "Episode:  36 | Episode Reward:  -169.86302185058594 | Episode Loss:  0\n",
      "Episode:  37 | Episode Reward:  -142.39222717285156 | Episode Loss:  0\n",
      "Episode:  38 | Episode Reward:  -269.5758972167969 | Episode Loss:  0\n",
      "Episode:  39 | Episode Reward:  -167.15457153320312 | Episode Loss:  0\n",
      "Episode:  40 | Episode Reward:  -112.88749694824219 | Episode Loss:  0\n",
      "Episode:  41 | Episode Reward:  -232.963623046875 | Episode Loss:  0\n",
      "Episode:  42 | Episode Reward:  -90.81687927246094 | Episode Loss:  0\n",
      "Episode:  43 | Episode Reward:  -187.77500915527344 | Episode Loss:  0\n",
      "Episode:  44 | Episode Reward:  -103.84516906738281 | Episode Loss:  0\n",
      "Episode:  45 | Episode Reward:  -184.3489990234375 | Episode Loss:  0\n",
      "Episode:  46 | Episode Reward:  -266.999267578125 | Episode Loss:  0\n",
      "Episode:  47 | Episode Reward:  -101.6352767944336 | Episode Loss:  0\n",
      "Episode:  48 | Episode Reward:  -132.08029174804688 | Episode Loss:  0\n",
      "Episode:  49 | Episode Reward:  -174.39576721191406 | Episode Loss:  0\n",
      "Episode:  50 | Episode Reward:  -174.997314453125 | Episode Loss:  0\n",
      "Episode 50 of 500:train reward -176.46, train loss 0.0000, eval reward -154.72,eval loss 0.00\n",
      "Episode:  51 | Episode Reward:  -140.80442810058594 | Episode Loss:  0\n",
      "Episode:  52 | Episode Reward:  -151.20242309570312 | Episode Loss:  0\n",
      "Episode:  53 | Episode Reward:  -186.9480743408203 | Episode Loss:  0\n",
      "Episode:  54 | Episode Reward:  -159.26870727539062 | Episode Loss:  0\n",
      "Episode:  55 | Episode Reward:  -274.3099060058594 | Episode Loss:  0\n",
      "Episode:  56 | Episode Reward:  -201.14967346191406 | Episode Loss:  0\n",
      "Episode:  57 | Episode Reward:  -110.8785400390625 | Episode Loss:  0\n",
      "Episode:  58 | Episode Reward:  -139.52169799804688 | Episode Loss:  0\n",
      "Episode:  59 | Episode Reward:  -104.90948486328125 | Episode Loss:  0\n",
      "Episode:  60 | Episode Reward:  -142.5914764404297 | Episode Loss:  0\n",
      "Episode:  61 | Episode Reward:  -161.5064239501953 | Episode Loss:  0\n",
      "Episode:  62 | Episode Reward:  -143.44296264648438 | Episode Loss:  0\n",
      "Episode:  63 | Episode Reward:  -98.43510437011719 | Episode Loss:  0\n",
      "Episode:  64 | Episode Reward:  -218.16644287109375 | Episode Loss:  0\n",
      "Episode:  65 | Episode Reward:  -167.041015625 | Episode Loss:  0\n",
      "Episode:  66 | Episode Reward:  -151.7280731201172 | Episode Loss:  0\n",
      "Episode:  67 | Episode Reward:  -188.33071899414062 | Episode Loss:  0\n",
      "pasuse\n",
      "Episode:  68 | Episode Reward:  -101.29283142089844 | Episode Loss:  0\n",
      "Episode:  69 | Episode Reward:  -188.0731201171875 | Episode Loss:  6448.264938354492\n",
      "Episode:  70 | Episode Reward:  -176.95455932617188 | Episode Loss:  5023.599212646484\n",
      "Episode:  71 | Episode Reward:  -70.77391815185547 | Episode Loss:  5996.0560302734375\n",
      "Episode:  72 | Episode Reward:  -31.12423324584961 | Episode Loss:  4365.1932373046875\n",
      "Episode:  73 | Episode Reward:  -16.88014030456543 | Episode Loss:  3523.52188873291\n",
      "Episode:  74 | Episode Reward:  -248.422119140625 | Episode Loss:  2889.9872970581055\n",
      "Episode:  75 | Episode Reward:  12.964057922363281 | Episode Loss:  2308.7030029296875\n",
      "Episode:  76 | Episode Reward:  -57.520782470703125 | Episode Loss:  2686.401596069336\n",
      "Episode:  77 | Episode Reward:  -121.7422866821289 | Episode Loss:  1824.8681602478027\n",
      "Episode:  78 | Episode Reward:  -63.41446304321289 | Episode Loss:  1489.4484367370605\n",
      "Episode:  79 | Episode Reward:  0.2536473274230957 | Episode Loss:  1355.1921882629395\n",
      "Episode:  80 | Episode Reward:  -361.39031982421875 | Episode Loss:  1249.285026550293\n",
      "Episode:  81 | Episode Reward:  -96.28306579589844 | Episode Loss:  2499.170711517334\n",
      "Episode:  82 | Episode Reward:  22.031131744384766 | Episode Loss:  1446.4959869384766\n",
      "Episode:  83 | Episode Reward:  -82.25074005126953 | Episode Loss:  1197.754482269287\n",
      "Episode:  84 | Episode Reward:  -58.22462463378906 | Episode Loss:  1007.4624614715576\n",
      "Episode:  85 | Episode Reward:  20.541458129882812 | Episode Loss:  923.6276226043701\n",
      "Episode:  86 | Episode Reward:  -170.3489227294922 | Episode Loss:  1565.2010650634766\n",
      "Episode:  87 | Episode Reward:  -110.18953704833984 | Episode Loss:  1166.2140426635742\n",
      "Episode:  88 | Episode Reward:  -30.357208251953125 | Episode Loss:  1008.6126823425293\n",
      "Episode:  89 | Episode Reward:  -129.25009155273438 | Episode Loss:  996.5021686553955\n",
      "Episode:  90 | Episode Reward:  -100.68620300292969 | Episode Loss:  890.3787097930908\n",
      "Episode:  91 | Episode Reward:  -55.23835754394531 | Episode Loss:  1940.6503372192383\n",
      "Episode:  92 | Episode Reward:  -156.13365173339844 | Episode Loss:  1315.9477577209473\n",
      "Episode:  93 | Episode Reward:  -212.6393280029297 | Episode Loss:  1010.9042510986328\n",
      "Episode:  94 | Episode Reward:  -374.48895263671875 | Episode Loss:  1137.9268856048584\n",
      "Episode:  95 | Episode Reward:  -117.67573547363281 | Episode Loss:  1018.3106822967529\n",
      "Episode:  96 | Episode Reward:  -264.2361755371094 | Episode Loss:  1752.4014282226562\n",
      "Episode:  97 | Episode Reward:  26.412128448486328 | Episode Loss:  1310.2305526733398\n",
      "Episode:  98 | Episode Reward:  -220.33755493164062 | Episode Loss:  1102.6840209960938\n",
      "Episode:  99 | Episode Reward:  -37.21678161621094 | Episode Loss:  1107.7593517303467\n",
      "Episode:  100 | Episode Reward:  -45.25497817993164 | Episode Loss:  1005.0622577667236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\Practical(1).ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_3a()\n",
      "\u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\Practical(1).ipynb Cell 36\u001b[0m in \u001b[0;36mtrain_3a\u001b[1;34m(training_loss_trace_path, training_rewards_trace_path, eval_loss_trace_path, eval_rewards_trace_path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     dqnagent_3a\u001b[39m.\u001b[39mupdate_target_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m small_cycle \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     eval_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([each_evaluation(env,dqnagent_3a,i) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     eval_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(eval_list,axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     eval_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(eval_list,axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\Practical(1).ipynb Cell 36\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     dqnagent_3a\u001b[39m.\u001b[39mupdate_target_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m small_cycle \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     eval_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([each_evaluation(env,dqnagent_3a,i) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     eval_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(eval_list,axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     eval_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(eval_list,axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\Practical(1).ipynb Cell 36\u001b[0m in \u001b[0;36meach_evaluation\u001b[1;34m(env, agent, seed)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state, \u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/DQN/Miniproject_DQN/Practical%281%29.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\epidemic_env\\env.py:209\u001b[0m, in \u001b[0;36mEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdyn\u001b[39m.\u001b[39mcities:\n\u001b[0;32m    208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdyn\u001b[39m.\u001b[39mset_action(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_action(action), c)\n\u001b[1;32m--> 209\u001b[0m _obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdyn\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_obs(_obs)\n\u001b[0;32m    212\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_reward(_obs)\n",
      "File \u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\epidemic_env\\dynamics.py:278\u001b[0m, in \u001b[0;36mModelDynamics.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m# step through a week of simulation to produce one environment step\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_step_length\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrate):\n\u001b[1;32m--> 278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_dyn()\n\u001b[0;32m    279\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepidemic_parameters(day\u001b[39m=\u001b[39mi\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrate)\n\u001b[0;32m    280\u001b[0m     _total_history\u001b[39m.\u001b[39mappend(params[\u001b[39m'\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32me:\\github_project\\DQN\\Miniproject_DQN\\epidemic_env\\dynamics.py:333\u001b[0m, in \u001b[0;36mModelDynamics.step_dyn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[39m# compute the derivative terms\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[39m# city - to city contagion\u001b[39;00m\n\u001b[0;32m    331\u001b[0m stoch_t0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\n\u001b[0;32m    332\u001b[0m     [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau_0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_tau_0), \u001b[39m0\u001b[39m])\n\u001b[1;32m--> 333\u001b[0m sum_term \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_isolated[c]\u001b[39m*\u001b[39mstoch_t0 \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49msum([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap\u001b[39m.\u001b[39;49mnodes[a][\u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap\u001b[39m.\u001b[39;49medges[(\n\u001b[0;32m    334\u001b[0m     a, c)][\u001b[39m'\u001b[39;49m\u001b[39mtau\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_isolated[a] \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m nx\u001b[39m.\u001b[39;49mneighbors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap, c)])\n\u001b[0;32m    336\u001b[0m \u001b[39m# incidence rate\u001b[39;00m\n\u001b[0;32m    337\u001b[0m stoch_alpha \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\n\u001b[0;32m    338\u001b[0m     [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_confined[c], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_alpha), \u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m<__array_function__ internals>:4\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2120\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2119\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 2120\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_3a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 3\n",
    "training_loss_trace_combine = []\n",
    "training_rewards_trace_combine = []\n",
    "eval_loss_trace_combine = []\n",
    "eval_rewards_trace_combine = []\n",
    "\n",
    "\n",
    "for i in range(n_experiments):\n",
    "\n",
    "    dqnagent_3a = DQNAgent(env, gamma = gamma, policy_net = policy_net, target_net = target_net, optimizer = optimizer, device = device, epsilon=0.7)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'=====================================Experiment {i+1} of {n_experiments}================================')\n",
    "    train_3a(training_loss_trace_path = f'./model_storage/3a_training_loss_trace_{i}.json',training_rewards_trace_path= f'./model_storage/3a_training_rewards_trace_{i}.json', eval_loss_trace_path = f'./model_storage/3a_eval_loss_trace_{i}.json', eval_rewards_trace_path = f'./model_storage/3a_eval_rewards_trace_{i}.json',)\n",
    "print(\"=====================================Experiment Done================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902705f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the three experiments\n",
    "def plot_trace():\n",
    "    \n",
    "\n",
    "    # import training and eval rewards trace from json\n",
    "    training_rewards_trace_combine = []\n",
    "    evaluation_rewards_trace_combine = []\n",
    "\n",
    "    for i in range(3):\n",
    "        with open(f'3a_training_rewards_trace_{i}.json', 'r') as f:\n",
    "            training_rewards_trace_combine.append(json.load(f))\n",
    "        with open(f'3a_eval_rewards_trace_{i}.json', 'r') as f:\n",
    "            evaluation_rewards_trace_combine.append(json.load(f))\n",
    "\n",
    "    small_cycle = 50\n",
    "    training_episode = np.arange(1,len(training_rewards_trace_combine[0])+1)\n",
    "    training_episodes = np.stack(training_episode for _  in range(len(training_loss_trace_combine)) )\n",
    "    training_rewards_trace_combine = np.array(training_rewards_trace_combine)\n",
    "    plt.scatter(training_episodes, training_rewards_trace_combine, s = 1, c = 'b', label = 'training')\n",
    "\n",
    "    evaluation_episode = np.arange(0,len(training_rewards_trace_combine[0]),small_cycle) + small_cycle\n",
    "    eva\n",
    "                                   \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
